import requests
from io import BytesIO
import queue


class Audio_generator:
    def __init__(self,
                 #  gpt_path: str,
                 #  sovits_path: str,
                 port: int = 9880,
                 ) -> None:

        # self.gpt_path = gpt_path
        # self.sovits_path = sovits_path
        self.port = port

        self.event = queue.Queue()
        self.result = queue.Queue()
        self.ready = False

    def gen(self, text: str, language: str = "ja", refer_data: dict = {}, cut_method: str = "cut3"):
        """
        cut0: ä¸åˆ‡
        cut1: å‡‘å››å¥ä¸€åˆ‡
        cut2: å‡‘50å­—ä¸€åˆ‡
        cut3: æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡
        cut4: æŒ‰è‹±æ–‡å¥å·.åˆ‡
        cut5: æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡
        refer_data: {"refer_wav_path": "", "prompt_text": "", "prompt_language": ""}
        language and prompt_language:
        "all_zh",#å…¨éƒ¨æŒ‰ä¸­æ–‡è¯†åˆ«
        "en",#å…¨éƒ¨æŒ‰è‹±æ–‡è¯†åˆ«#######ä¸å˜
        "all_ja",#å…¨éƒ¨æŒ‰æ—¥æ–‡è¯†åˆ«
        "all_yue",#å…¨éƒ¨æŒ‰ä¸­æ–‡è¯†åˆ«
        "all_ko",#å…¨éƒ¨æŒ‰éŸ©æ–‡è¯†åˆ«
        "zh",#æŒ‰ä¸­è‹±æ··åˆè¯†åˆ«####ä¸å˜
        "ja",#æŒ‰æ—¥è‹±æ··åˆè¯†åˆ«####ä¸å˜
        "yue",#æŒ‰ç²¤è‹±æ··åˆè¯†åˆ«####ä¸å˜
        "ko",#æŒ‰éŸ©è‹±æ··åˆè¯†åˆ«####ä¸å˜
        "auto",#å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§
        "auto_yue",#å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§


        """
        if text and refer_data:
            payload = {
                # str.(required) text to be synthesized
                "text": text,
                # str.(required) language of the text to be synthesized
                "text_lang": language,
                # str.(required) reference audio path
                "ref_audio_path": refer_data.get("refer_wav_path"),
                # list.(optional) auxiliary reference audio paths for multi-speaker synthesis
                "aux_ref_audio_paths": [],
                # str.(optional) prompt text for the reference audio
                "prompt_text": refer_data.get("prompt_text"),
                # str.(required) language of the prompt text for the reference audio
                "prompt_lang": refer_data.get("prompt_language"),
                "top_k": 5,                   # int. top k sampling
                "top_p": 1,                   # float. top p sampling
                "temperature": 1,             # float. temperature for sampling
                # str. text split method, see text_segmentation_method.py for details.
                "text_split_method": cut_method,
                "batch_size": 1,              # int. batch size for inference
                # float. threshold for batch splitting.
                "batch_threshold": 0.75,
                # bool. whether to split the batch into multiple buckets.
                "split_bucket": True,
                # bool. step by step return the audio fragment.
                "return_fragment": False,
                # float. control the speed of the synthesized audio.
                "speed_factor": 1.0,
                # bool. whether to return a streaming response.
                "streaming_mode": False,
                # int. random seed for reproducibility.
                "seed": -1,
                # bool. whether to use parallel inference.
                "parallel_infer": True,
                # float. repetition penalty for T2S model.
                "repetition_penalty": 1.35
            }
        else:
            pass  # todo: use bert to get emotion
        with requests.post(url=f"http://127.0.0.1:{self.port}/tts", json=payload) as response:
            if response.status_code == 200:
                audio = BytesIO()
                audio.write(response.content)
                audio.seek(0)
                # self.result.put(audio.getvalue())
                self.result.put(audio)
                self.ready = True
            else:
                self.result.put(response.json())
                self.ready = True

    def call_method(self, method_name: str, *args, **kwargs):
        if hasattr(self, method_name):
            method = getattr(self, method_name)
            if callable(method):
                print(f"call method {method_name}")
                return method(*args, **kwargs)
            else:
                print(f"{method_name} not callable")
                return None
        else:
            print(f"{method_name} not found")
            return None

    def process_event(self, event):
        """
        å¤„ç†äº‹ä»¶ï¼Œæ ¹æ®äº‹ä»¶ç±»å‹è°ƒç”¨ç›¸åº”çš„æ–¹æ³•ã€‚

        å‚æ•°:
            event: å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆæ–¹æ³•åï¼‰æˆ–å…ƒç»„ (method_name, args, kwargs)ã€‚

        è¿”å›:
            æ–¹æ³•è°ƒç”¨çš„ç»“æœæˆ– Noneã€‚
        """
        if isinstance(event, str):  # äº‹ä»¶æ˜¯ä¸€ä¸ªæ–¹æ³•åå­—ç¬¦ä¸²
            return self.call_method(event)
        elif isinstance(event, tuple) and len(event) > 0:  # äº‹ä»¶æ˜¯ä¸€ä¸ªå…ƒç»„
            method_name = event[0]  # ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ–¹æ³•å
            if len(event) == 1:  # åªæœ‰æ–¹æ³•åï¼Œæ— å‚æ•°
                return self.call_method(method_name)
            elif len(event) == 2:  # æ–¹æ³•åå’Œå‚æ•°
                if isinstance(event[1], dict):  # ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å­—å…¸ï¼Œä½œä¸º kwargs å¤„ç†
                    return self.call_method(method_name, **event[1])
                elif isinstance(event[1], tuple):  # ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å…ƒç»„ï¼Œä½œä¸º args å¤„ç†
                    return self.call_method(method_name, *event[1])
            elif len(event) == 3:  # æ–¹æ³•åã€args å’Œ kwargs
                args = event[1] if isinstance(event[1], tuple) else ()
                kwargs = event[2] if isinstance(event[2], dict) else {}
                return self.call_method(method_name, *args, **kwargs)
        return None

    def run(self):
        while True:
            event = self.event.get()
            if event:
                self.process_event(event)


if __name__ == "__main__":
    import sounddevice as sd
    import soundfile as sf
    import threading
    import time

    generator = Audio_generator(port=9880)
    t = threading.Thread(target=generator.run)
    t.start()
    refer_data = {"refer_wav_path": r"D:\GPT-SoVITS-v2-240821\GPT-SoVITS-v2-240821\output\noa\noa_153.wav",
                  "prompt_text": "ãã‚Œãªã‚‰ã‚†ã†ã‹ã¡ã‚ƒã‚“ã®å£°ãŒæµã‚Œã‚‹ç›®è¦šã¾ã—æ™‚è¨ˆã¨ã‹ã„ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã­",
                  "prompt_language": "ja"}
    generator.event.put(
        ("gen", {"text": "ã“ã‚“ã«ã¡ã¯ã€ğŸ˜€ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼Ÿ", "language": "ja", "refer_data": refer_data}))
    while not generator.ready:
        time.sleep(0.1)
    generator.ready = False
    audio = generator.result.get()
    data, samplerate = sf.read(audio)
    sd.play(data, samplerate)
    sd.wait()
